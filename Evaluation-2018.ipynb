{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015f6b32-4106-48bc-b296-bcdf8fed5137",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bf5ec2-1f47-4425-8169-fc0be95c2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce79d91-7719-4a56-8e8f-3c98fed9b01e",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f430fda5-27a7-4b72-8432-3e1f9833faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Tot Bwd Pkts</th>\n",
       "      <th>TotLen Fwd Pkts</th>\n",
       "      <th>TotLen Bwd Pkts</th>\n",
       "      <th>Fwd Pkt Len Max</th>\n",
       "      <th>Fwd Pkt Len Min</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 10:35:25</td>\n",
       "      <td>115977940</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>2188</td>\n",
       "      <td>22138</td>\n",
       "      <td>918</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>49290.083333</td>\n",
       "      <td>95370.791302</td>\n",
       "      <td>352133</td>\n",
       "      <td>21697</td>\n",
       "      <td>9.615537e+06</td>\n",
       "      <td>1.361640e+06</td>\n",
       "      <td>10009383</td>\n",
       "      <td>5291754</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3128</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 09:33:53</td>\n",
       "      <td>1002481</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 03:32:20</td>\n",
       "      <td>60108102</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>252</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>77682.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77682</td>\n",
       "      <td>77682</td>\n",
       "      <td>5.993778e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>59937783</td>\n",
       "      <td>59937783</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>15/02/2018 10:53:26</td>\n",
       "      <td>13854</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>198</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 08:59:56</td>\n",
       "      <td>326</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>50832</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 01:35:49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 10:10:22</td>\n",
       "      <td>54306882</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>15/02/2018 09:50:52</td>\n",
       "      <td>119508</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>210</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>15/02/2018 02:40:51</td>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>131</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>15/02/2018 12:29:21</td>\n",
       "      <td>6702790</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>1037</td>\n",
       "      <td>51397</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>706758.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>706758</td>\n",
       "      <td>706758</td>\n",
       "      <td>5.995843e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5995843</td>\n",
       "      <td>5995843</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
       "0           443         6  15/02/2018 10:35:25      115977940            28   \n",
       "1          3128         6  15/02/2018 09:33:53        1002481             2   \n",
       "2           443         6  15/02/2018 03:32:20       60108102             4   \n",
       "3            53        17  15/02/2018 10:53:26          13854             1   \n",
       "4           443         6  15/02/2018 08:59:56            326             3   \n",
       "...         ...       ...                  ...            ...           ...   \n",
       "59995     50832         6  15/02/2018 01:35:49              1             2   \n",
       "59996        80         6  15/02/2018 10:10:22       54306882             2   \n",
       "59997        53        17  15/02/2018 09:50:52         119508             2   \n",
       "59998        53        17  15/02/2018 02:40:51            925             1   \n",
       "59999       443         6  15/02/2018 12:29:21        6702790            20   \n",
       "\n",
       "       Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
       "0                30             2188            22138              918   \n",
       "1                 0                0                0                0   \n",
       "2                 2              147              252               74   \n",
       "3                 1               44              198               44   \n",
       "4                 0               53                0               53   \n",
       "...             ...              ...              ...              ...   \n",
       "59995             0                0                0                0   \n",
       "59996             0                0                0                0   \n",
       "59997             2               90              210               45   \n",
       "59998             1               65              131               65   \n",
       "59999            50             1037            51397              364   \n",
       "\n",
       "       Fwd Pkt Len Min  ...  Fwd Seg Size Min    Active Mean    Active Std  \\\n",
       "0                    0  ...                20   49290.083333  95370.791302   \n",
       "1                    0  ...                40       0.000000      0.000000   \n",
       "2                    0  ...                20   77682.000000      0.000000   \n",
       "3                   44  ...                 8       0.000000      0.000000   \n",
       "4                    0  ...                20       0.000000      0.000000   \n",
       "...                ...  ...               ...            ...           ...   \n",
       "59995                0  ...                20       0.000000      0.000000   \n",
       "59996                0  ...                20       0.000000      0.000000   \n",
       "59997               45  ...                 8       0.000000      0.000000   \n",
       "59998               65  ...                 8       0.000000      0.000000   \n",
       "59999                0  ...                20  706758.000000      0.000000   \n",
       "\n",
       "       Active Max  Active Min     Idle Mean      Idle Std  Idle Max  Idle Min  \\\n",
       "0          352133       21697  9.615537e+06  1.361640e+06  10009383   5291754   \n",
       "1               0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "2           77682       77682  5.993778e+07  0.000000e+00  59937783  59937783   \n",
       "3               0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "4               0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "...           ...         ...           ...           ...       ...       ...   \n",
       "59995           0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "59996           0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "59997           0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "59998           0           0  0.000000e+00  0.000000e+00         0         0   \n",
       "59999      706758      706758  5.995843e+06  0.000000e+00   5995843   5995843   \n",
       "\n",
       "        Label  \n",
       "0      Benign  \n",
       "1      Benign  \n",
       "2      Benign  \n",
       "3      Benign  \n",
       "4      Benign  \n",
       "...       ...  \n",
       "59995  Benign  \n",
       "59996  Benign  \n",
       "59997  Benign  \n",
       "59998  Benign  \n",
       "59999  Benign  \n",
       "\n",
       "[60000 rows x 80 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"E:\\train\\sampled_data_2018.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83902476-4362-419c-8cd1-e60977b529ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dst Port          4836\n",
       "Protocol             3\n",
       "Timestamp        23647\n",
       "Flow Duration    35199\n",
       "Tot Fwd Pkts       202\n",
       "                 ...  \n",
       "Idle Mean         9330\n",
       "Idle Std          5898\n",
       "Idle Max          9069\n",
       "Idle Min          9089\n",
       "Label                3\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Unique Values\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd7a372d-8182-4aa7-b1ca-feb2b58f455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dst Port         0\n",
       "Protocol         0\n",
       "Timestamp        0\n",
       "Flow Duration    0\n",
       "Tot Fwd Pkts     0\n",
       "                ..\n",
       "Idle Mean        0\n",
       "Idle Std         0\n",
       "Idle Max         0\n",
       "Idle Min         0\n",
       "Label            0\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eff2e6d4-37ad-43c6-a3c7-6ed9263472d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for each class:\n",
      "Label\n",
      "Benign                   56991\n",
      "DoS attacks-GoldenEye     2332\n",
      "DoS attacks-Slowloris      677\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"E:\\train\\sampled_data_2018.csv\")\n",
    "\n",
    "# Get the value counts for the 'Label' column\n",
    "label_counts = df['Label'].value_counts()\n",
    "\n",
    "# Print the value counts\n",
    "print(\"Value counts for each class:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7071dc-eb44-48d0-98e2-fe0a062a463c",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1327ada0-c2f1-437f-a14d-d2e9b7731d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
      "0       443         6  15/02/2018 10:35:25      115977940            28   \n",
      "1      3128         6  15/02/2018 09:33:53        1002481             2   \n",
      "2       443         6  15/02/2018 03:32:20       60108102             4   \n",
      "3        53        17  15/02/2018 10:53:26          13854             1   \n",
      "4       443         6  15/02/2018 08:59:56            326             3   \n",
      "\n",
      "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
      "0            30             2188            22138              918   \n",
      "1             0                0                0                0   \n",
      "2             2              147              252               74   \n",
      "3             1               44              198               44   \n",
      "4             0               53                0               53   \n",
      "\n",
      "   Fwd Pkt Len Min  ...  Fwd Seg Size Min   Active Mean    Active Std  \\\n",
      "0                0  ...                20  49290.083333  95370.791302   \n",
      "1                0  ...                40      0.000000      0.000000   \n",
      "2                0  ...                20  77682.000000      0.000000   \n",
      "3               44  ...                 8      0.000000      0.000000   \n",
      "4                0  ...                20      0.000000      0.000000   \n",
      "\n",
      "   Active Max  Active Min     Idle Mean      Idle Std  Idle Max  Idle Min  \\\n",
      "0      352133       21697  9.615537e+06  1.361640e+06  10009383   5291754   \n",
      "1           0           0  0.000000e+00  0.000000e+00         0         0   \n",
      "2       77682       77682  5.993778e+07  0.000000e+00  59937783  59937783   \n",
      "3           0           0  0.000000e+00  0.000000e+00         0         0   \n",
      "4           0           0  0.000000e+00  0.000000e+00         0         0   \n",
      "\n",
      "    Label  \n",
      "0  Benign  \n",
      "1  Benign  \n",
      "2  Benign  \n",
      "3  Benign  \n",
      "4  Benign  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "\n",
      "Normalized Data:\n",
      "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
      "0 -0.386946 -0.666735  15/02/2018 10:35:25       3.018313      0.684984   \n",
      "1 -0.231198 -0.666735  15/02/2018 09:33:53      -0.404010     -0.098403   \n",
      "2 -0.386946 -0.666735  15/02/2018 03:32:20       1.355309     -0.038142   \n",
      "3 -0.409568  1.399279  15/02/2018 10:53:26      -0.433437     -0.128533   \n",
      "4 -0.386946 -0.666735  15/02/2018 08:59:56      -0.433840     -0.068272   \n",
      "\n",
      "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
      "0      0.238390         0.047395         0.121296         2.533724   \n",
      "1     -0.070528        -0.013696        -0.039245        -0.507430   \n",
      "2     -0.049933        -0.009591        -0.037418        -0.262282   \n",
      "3     -0.060231        -0.012467        -0.037810        -0.361667   \n",
      "4     -0.070528        -0.012216        -0.039245        -0.331851   \n",
      "\n",
      "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
      "0        -0.588964  ...          0.459628    -0.067468    0.130129   \n",
      "1        -0.588964  ...          3.117764    -0.118969   -0.133049   \n",
      "2        -0.588964  ...          0.459628    -0.037802   -0.133049   \n",
      "3         1.239581  ...         -1.135254    -0.118969   -0.133049   \n",
      "4        -0.588964  ...          0.459628    -0.118969   -0.133049   \n",
      "\n",
      "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min   Label  \n",
      "0    0.140690   -0.071207   0.356571  0.457959  0.349149  0.075858  Benign  \n",
      "1   -0.152380   -0.095573  -0.308100 -0.114787 -0.313300 -0.295989  Benign  \n",
      "2   -0.087728   -0.008336   3.835080 -0.114787  3.653549  3.915783  Benign  \n",
      "3   -0.152380   -0.095573  -0.308100 -0.114787 -0.313300 -0.295989  Benign  \n",
      "4   -0.152380   -0.095573  -0.308100 -0.114787 -0.313300 -0.295989  Benign  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"E:\\train\\sampled_data_2018.csv\")\n",
    "\n",
    "# Check for missing values and handle them (replace with mean or drop, depending on your preference)\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Check for infinite values and replace them with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values after handling missing and infinite values\n",
    "df = df.dropna()\n",
    "\n",
    "# Select only the numerical columns for normalization\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Create a copy of the DataFrame for printing the values before normalization\n",
    "original_df = df.copy()\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply Z-score normalization to the numerical columns\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Save the normalized DataFrame to a new CSV file\n",
    "normalized_output_file_path = r\"E:\\train\\normalized_data.csv\"\n",
    "df.to_csv(normalized_output_file_path, index=False)\n",
    "\n",
    "# Index=False is used to prevent pandas from writing row indices to the CSV file\n",
    "\n",
    "# Print the original and normalized values\n",
    "print(\"Original Data:\")\n",
    "print(original_df.head())  # Print the first few rows of the original DataFrame\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(df.head())  # Print the first few rows of the normalized DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888afc2d-b408-4a09-820b-20254ba2d2db",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9686d076-a7f4-4702-87df-e2dce6751df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features with Labels:\n",
      "   SVD_Component_1  SVD_Component_2  SVD_Component_3  SVD_Component_4  \\\n",
      "0         9.415213         2.294719        -1.414894        -0.709774   \n",
      "1        -1.548503        -0.233991        -0.267681         2.287735   \n",
      "2         6.204385       -11.415696         0.865256        -1.989816   \n",
      "3        -1.917640         1.267475         0.643055        -3.444450   \n",
      "4        -2.095726        -0.344502         0.257323         1.608610   \n",
      "\n",
      "   SVD_Component_5  SVD_Component_6  SVD_Component_7  SVD_Component_8  \\\n",
      "0         0.072641        -1.401322        -1.559276         2.331749   \n",
      "1        -0.587621        -0.518011         0.949385        -1.037395   \n",
      "2        -3.676117         4.104402        -6.338533        -6.116195   \n",
      "3         0.117429         0.049833        -0.440399         0.355889   \n",
      "4        -0.205642         0.221452         0.346105        -0.356100   \n",
      "\n",
      "   SVD_Component_9  SVD_Component_10   Label  \n",
      "0         2.282646          0.841691  Benign  \n",
      "1         0.888890          0.253593  Benign  \n",
      "2        -2.539196          3.243171  Benign  \n",
      "3        -0.275536          0.089772  Benign  \n",
      "4         0.760294          0.196761  Benign  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load the normalized data from the CSV file\n",
    "normalized_data_path = r\"E:\\train\\normalized_data.csv\"\n",
    "df_normalized = pd.read_csv(normalized_data_path)\n",
    "\n",
    "# Separate labels from features\n",
    "labels = df_normalized['Label']\n",
    "features = df_normalized.drop(columns=['Label','Timestamp'])\n",
    "\n",
    "# Perform SVD on the feature matrix\n",
    "n_components = 10  # Number of components to keep\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "extracted_features = svd.fit_transform(features)\n",
    "\n",
    "# Create a DataFrame for the extracted features\n",
    "df_extracted_features = pd.DataFrame(extracted_features, columns=[f'SVD_Component_{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "df_extracted_features['Label'] = labels\n",
    "\n",
    "# Save the extracted features with labels to a new CSV file\n",
    "extracted_features_output_path = r\"E:\\train\\extracted_features.csv\"\n",
    "df_extracted_features.to_csv(extracted_features_output_path, index=False)\n",
    "\n",
    "# Print the values of the extracted features with labels\n",
    "print(\"Extracted Features with Labels:\")\n",
    "print(df_extracted_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4f985-7aa2-4088-a961-c2c5f1a8be41",
   "metadata": {},
   "source": [
    "# Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7a225a7-31da-4150-825b-e630f29cf54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features with Encoded Labels:\n",
      "   SVD_Component_1  SVD_Component_2  SVD_Component_3  SVD_Component_4  \\\n",
      "0         9.415213         2.294719        -1.414894        -0.709774   \n",
      "1        -1.548503        -0.233991        -0.267681         2.287735   \n",
      "2         6.204385       -11.415696         0.865256        -1.989816   \n",
      "3        -1.917640         1.267475         0.643055        -3.444450   \n",
      "4        -2.095726        -0.344502         0.257323         1.608610   \n",
      "\n",
      "   SVD_Component_5  SVD_Component_6  SVD_Component_7  SVD_Component_8  \\\n",
      "0         0.072641        -1.401322        -1.559276         2.331749   \n",
      "1        -0.587621        -0.518011         0.949385        -1.037395   \n",
      "2        -3.676117         4.104402        -6.338533        -6.116195   \n",
      "3         0.117429         0.049833        -0.440399         0.355889   \n",
      "4        -0.205642         0.221452         0.346105        -0.356100   \n",
      "\n",
      "   SVD_Component_9  SVD_Component_10  Label  \n",
      "0         2.282646          0.841691      0  \n",
      "1         0.888890          0.253593      0  \n",
      "2        -2.539196          3.243171      0  \n",
      "3        -0.275536          0.089772      0  \n",
      "4         0.760294          0.196761      0  \n",
      "Encoded features saved to: E:\\train\\encoded_features.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define label encoding dictionary\n",
    "label_encoding = {\n",
    "    'Benign': 0,\n",
    "    'DoS attacks-GoldenEye': 1,\n",
    "    'DoS attacks-Slowloris': 1\n",
    "}\n",
    "\n",
    "# Apply label encoding to the 'Label' column\n",
    "df_extracted_features['Label'] = df_extracted_features['Label'].map(label_encoding)\n",
    "\n",
    "# Print the values of the extracted features with encoded labels\n",
    "print(\"Extracted Features with Encoded Labels:\")\n",
    "print(df_extracted_features.head())\n",
    "\n",
    "# Save the DataFrame with encoded labels to a new CSV file\n",
    "encoded_features_output_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features.to_csv(encoded_features_output_path, index=False)\n",
    "\n",
    "# Print the path where the encoded features are saved\n",
    "print(f\"Encoded features saved to: {encoded_features_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4122f553-efb7-48c1-96cd-1e58a5997b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for each class:\n",
      "Label\n",
      "0    56556\n",
      "1     3009\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"E:\\train\\encoded_features.csv\")\n",
    "\n",
    "# Get the value counts for the 'Label' column\n",
    "label_counts = df['Label'].value_counts()\n",
    "\n",
    "# Print the value counts\n",
    "print(\"Value counts for each class:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbee85-c6fb-4999-a1b5-d587d3839809",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00477851-bfde-436f-9bc3-f38ad51f4240",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee1a3ddb-37ae-4989-bd01-caec38156ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995802904390162\n",
      "F1 Score: 0.9958088851634534\n",
      "Detection Rate : 0.9933110367892977\n",
      "Precision: 0.9983193277310924\n",
      "Recall: 0.9933110367892977\n",
      "AUC Score: 0.9999759841687641\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11315\n",
      "           1       1.00      0.99      1.00       598\n",
      "\n",
      "    accuracy                           1.00     11913\n",
      "   macro avg       1.00      1.00      1.00     11913\n",
      "weighted avg       1.00      1.00      1.00     11913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "class ThompsonSamplingMultiArmedBandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
    "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Multi-Armed Bandit\n",
    "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
    "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "for _ in range(len(X_train)):\n",
    "    arm = bandit.choose_arm()\n",
    "    reward = 1 if y_train.iloc[_] == arm else 0\n",
    "    bandit.update(arm, reward)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the testing set\n",
    "y_pred_proba = random_forest_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold the probabilities to get binary predictions\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Parse classification report to get F1 score and detection rate\n",
    "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "f1_score = classification_dict['1']['f1-score']\n",
    "detection_rate = classification_dict['1']['recall']\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Detection Rate : {detection_rate}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC Score: {auc_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038635a0-6c42-4ed8-8dea-b59e3fe024a7",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "190752a4-28e4-4b1d-a6f0-3fd4e62c5584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9931167631998656\n",
      "F1 Score: 0.930976430976431\n",
      "Detection Rate : 0.9247491638795987\n",
      "Precision: 0.9372881355932203\n",
      "Recall: 0.9247491638795987\n",
      "AUC Score: 0.9967022199495446\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11315\n",
      "           1       0.94      0.92      0.93       598\n",
      "\n",
      "    accuracy                           0.99     11913\n",
      "   macro avg       0.97      0.96      0.96     11913\n",
      "weighted avg       0.99      0.99      0.99     11913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "class ThompsonSamplingMultiArmedBandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
    "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Multi-Armed Bandit\n",
    "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
    "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "for _ in range(len(X_train)):\n",
    "    arm = bandit.choose_arm()\n",
    "    reward = 1 if y_train.iloc[_] == arm else 0\n",
    "    bandit.update(arm, reward)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the testing set\n",
    "y_pred_proba = svm_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold the probabilities to get binary predictions\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Parse classification report to get F1 score and detection rate\n",
    "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "f1_score = classification_dict['1']['f1-score']\n",
    "detection_rate = classification_dict['1']['recall']\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Detection Rate : {detection_rate}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC Score: {auc_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e7103-cc76-4140-9021-fada80075549",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3fd5ff0-4611-4ed2-af95-d7f1a2c56b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9796860572483841\n",
      "F1 Score: 0.7716981132075472\n",
      "Detection Rate : 0.6839464882943144\n",
      "Precision: 0.8852813852813853\n",
      "Recall: 0.6839464882943144\n",
      "AUC Score: 0.9672292529081323\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     11315\n",
      "           1       0.89      0.68      0.77       598\n",
      "\n",
      "    accuracy                           0.98     11913\n",
      "   macro avg       0.93      0.84      0.88     11913\n",
      "weighted avg       0.98      0.98      0.98     11913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "class ThompsonSamplingMultiArmedBandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
    "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Multi-Armed Bandit\n",
    "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
    "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "for _ in range(len(X_train)):\n",
    "    arm = bandit.choose_arm()\n",
    "    reward = 1 if y_train.iloc[_] == arm else 0\n",
    "    bandit.update(arm, reward)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the testing set\n",
    "y_pred_proba = logistic_regression_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold the probabilities to get binary predictions\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Parse classification report to get F1 score and detection rate\n",
    "classification_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "f1_score = classification_dict['1']['f1-score']\n",
    "detection_rate = classification_dict['1']['recall']\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Detection Rate : {detection_rate}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC Score: {auc_score}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20c007-4066-41b8-8720-3c762adf1feb",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4546c705-51af-41cf-9093-2a72bb568c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1192/1192 [==============================] - 9s 6ms/step - loss: 0.0467 - accuracy: 0.9858 - val_loss: 0.0164 - val_accuracy: 0.9958\n",
      "Epoch 2/10\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 0.0164 - accuracy: 0.9960 - val_loss: 0.0163 - val_accuracy: 0.9971\n",
      "Epoch 3/10\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 0.0124 - accuracy: 0.9971 - val_loss: 0.0115 - val_accuracy: 0.9977\n",
      "Epoch 4/10\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.0092 - val_accuracy: 0.9979\n",
      "Epoch 5/10\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.0083 - val_accuracy: 0.9959\n",
      "Epoch 6/10\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0071 - val_accuracy: 0.9987\n",
      "Epoch 7/10\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0061 - val_accuracy: 0.9988\n",
      "Epoch 8/10\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.0117 - val_accuracy: 0.9965\n",
      "Epoch 9/10\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
      "Epoch 10/10\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.0053 - val_accuracy: 0.9988\n",
      "373/373 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.9986569294048518\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11315\n",
      "           1       0.98      0.99      0.99       598\n",
      "\n",
      "    accuracy                           1.00     11913\n",
      "   macro avg       0.99      0.99      0.99     11913\n",
      "weighted avg       1.00      1.00      1.00     11913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class ThompsonSamplingMultiArmedBandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.ones(n_arms)  # Initialize alpha parameters to 1\n",
    "        self.beta = np.ones(n_arms)   # Initialize beta parameters to 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        samples = np.random.beta(self.alpha, self.beta)  # Thompson sampling\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Multi-Armed Bandit\n",
    "n_arms = len(np.unique(y_train))  # Number of unique classes\n",
    "bandit = ThompsonSamplingMultiArmedBandit(n_arms)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "for _ in range(len(X_train)):\n",
    "    arm = bandit.choose_arm()\n",
    "    reward = 1 if y_train.iloc[_] == arm else 0\n",
    "    bandit.update(arm, reward)\n",
    "\n",
    "# Define and train a Deep Neural Network (DNN) using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_arms, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de181b42-de4a-4613-8819-6a0f9338f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 [==============================] - 1s 2ms/step\n",
      "Precision: 0.9883333333333333\n",
      "Recall: 0.9916387959866221\n",
      "F1 Score: 0.989983305509182\n",
      "Detection Rate (Recall): 0.9916387959866221\n",
      "AUC Score: 0.9992607188196921\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities on the testing set\n",
    "y_pred_proba = model.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, F1 score, detection rate (recall), and AUC score\n",
    "precision_recall_f1 = classification_report(y_test, np.argmax(y_pred_proba, axis=1), output_dict=True)\n",
    "precision = precision_recall_f1['1']['precision']\n",
    "recall = precision_recall_f1['1']['recall']\n",
    "f1_score = precision_recall_f1['1']['f1-score']\n",
    "detection_rate = recall\n",
    "\n",
    "# Calculate AUC score for each class separately\n",
    "auc_scores = []\n",
    "for i in range(len(np.unique(y_train))):\n",
    "    auc_scores.append(roc_auc_score((y_test == i).astype(int), y_pred_proba[:, i]))\n",
    "\n",
    "# Average AUC scores across all classes\n",
    "auc_score = np.mean(auc_scores)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Detection Rate (Recall): {detection_rate}\")\n",
    "print(f\"AUC Score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee418c4-4ec2-42cf-8313-08fad0475a27",
   "metadata": {},
   "source": [
    "# IDS-Anta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f91259-5809-4cf3-95d0-ed7987394f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490/1490 [==============================] - 10s 6ms/step - loss: 0.0502 - accuracy: 0.9850\n",
      "1490/1490 [==============================] - 6s 4ms/step - loss: 0.0180 - accuracy: 0.9944\n",
      "Accuracy: 0.9995802904390162\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     11315\n",
      "           1       1.00      0.99      1.00       598\n",
      "\n",
      "    accuracy                           1.00     11913\n",
      "   macro avg       1.00      1.00      1.00     11913\n",
      "weighted avg       1.00      1.00      1.00     11913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MultiArmedBanditAntColonyOptimization:\n",
    "    def __init__(self, n_arms, n_ants):\n",
    "        self.n_arms = n_arms\n",
    "        self.n_ants = n_ants\n",
    "        self.arms = [SVC(probability=True), LogisticRegression(), RandomForestClassifier(), self.create_dnn_model()]\n",
    "        self.pheromone = np.ones(n_arms)\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "    def create_dnn_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def choose_arm(self):\n",
    "        probabilities = self.pheromone / np.sum(self.pheromone)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.n_arms)\n",
    "        else:\n",
    "            return np.random.choice(self.n_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.pheromone[arm] += reward\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Multi-Armed Bandit with Ant Colony Optimization\n",
    "n_arms = 4  # Number of classifiers\n",
    "n_ants = 10  # Number of ants\n",
    "bandit = MultiArmedBanditAntColonyOptimization(n_arms, n_ants)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "num_iterations = 10\n",
    "for _ in range(num_iterations):\n",
    "    for _ in range(n_ants):\n",
    "        arm = bandit.choose_arm()\n",
    "        classifier = bandit.arms[arm]\n",
    "        classifier.fit(X_train, y_train)\n",
    "        if classifier.__class__.__name__ != 'Sequential':\n",
    "            y_pred = classifier.predict(X_train)\n",
    "            reward = accuracy_score(y_train, y_pred)\n",
    "        else:\n",
    "            _, accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
    "            reward = accuracy\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "# Choose the best classifier\n",
    "best_arm = np.argmax(bandit.pheromone)\n",
    "best_classifier = bandit.arms[best_arm]\n",
    "\n",
    "# Evaluate the best classifier on the test set\n",
    "if best_classifier.__class__.__name__ != 'Sequential':\n",
    "    y_pred = best_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "else:\n",
    "    _, accuracy = best_classifier.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = (best_classifier.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc3329ea-89d1-4dc1-bfe2-0c92ce82d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 [==============================] - 4s 8ms/step\n",
      "373/373 [==============================] - 2s 5ms/step\n",
      "373/373 [==============================] - 1s 2ms/step\n",
      "373/373 [==============================] - 1s 2ms/step\n",
      "Evaluation Metrics :\n",
      "Precision: 0.9916107382550335\n",
      "Recall: 0.9882943143812709\n",
      "F1 Score: 0.9899497487437185\n",
      "AUC Score: 0.9994085454978076\n",
      "Detection Rate : 0.9882943143812709\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MultiArmedBanditAntColonyOptimization:\n",
    "    def __init__(self, n_arms, n_ants):\n",
    "        self.n_arms = n_arms\n",
    "        self.n_ants = n_ants\n",
    "        self.arms = [SVC(probability=True), LogisticRegression(), RandomForestClassifier(), self.create_dnn_model()]\n",
    "        self.pheromone = np.ones(n_arms)\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "    def create_dnn_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def choose_arm(self):\n",
    "        probabilities = self.pheromone / np.sum(self.pheromone)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.n_arms)\n",
    "        else:\n",
    "            return np.random.choice(self.n_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.pheromone[arm] += reward\n",
    "\n",
    "# Load the extracted features and labels from the CSV file\n",
    "extracted_features_path = r\"E:\\train\\encoded_features.csv\"\n",
    "df_extracted_features = pd.read_csv(extracted_features_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_extracted_features.drop(columns=['Label'])\n",
    "y = df_extracted_features['Label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Multi-Armed Bandit with Ant Colony Optimization\n",
    "n_arms = 4  # Number of classifiers\n",
    "n_ants = 10  # Number of ants\n",
    "bandit = MultiArmedBanditAntColonyOptimization(n_arms, n_ants)\n",
    "\n",
    "# Train the Multi-Armed Bandit\n",
    "num_iterations = 10\n",
    "for _ in range(num_iterations):\n",
    "    for _ in range(n_ants):\n",
    "        arm = bandit.choose_arm()\n",
    "        classifier = bandit.arms[arm]\n",
    "        classifier.fit(X_train, y_train)\n",
    "        if classifier.__class__.__name__ != 'Sequential':\n",
    "            y_pred = classifier.predict(X_train)\n",
    "            reward = accuracy_score(y_train, y_pred)\n",
    "        else:\n",
    "            _, accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
    "            reward = accuracy\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "# Choose the best classifier\n",
    "best_arm = np.argmax(bandit.pheromone)\n",
    "best_classifier = bandit.arms[best_arm]\n",
    "\n",
    "# Evaluate the best classifier on the test set\n",
    "if best_classifier.__class__.__name__ != 'Sequential':\n",
    "    y_pred = best_classifier.predict(X_test)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "else:\n",
    "    _, accuracy = best_classifier.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = (best_classifier.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Compute detection rate (equivalent to recall)\n",
    "detection_rate = recall\n",
    "\n",
    "# Print precision, recall, detection rate, F1 score, and AUC score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Detection Rate:\", detection_rate)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
